{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## thoughts\n",
    "* 3dGS render looks better than 2dGS\n",
    "* it seems that good reconstructions cun be achieved also with 3dGS\n",
    "* for the flies, the 2dGS gives good results, need to retest the 3d - in time\n",
    "* the reconstruction in 2D is based on the depth map alone\n",
    "* why not treat the 3dGS as havving a direction in the smallest scale? \n",
    "    * problematic with spheres, but if initiating a pita one and adding normal regularization, why would it become a sphere?\n",
    "* everything is pixelwise - there is an article with patches\n",
    "* examining other views in the context of reconstruction (epipolar, saw something, not sure were, need to find)\n",
    "* something with the lightfield? the rendering of 3d is really good\n",
    "* send a ray and sample (maybe saw something, need to find)? + multiview - get the most similiar feature on the ray \n",
    "* combine stereo reconstruction in the optimization process\n",
    "\n",
    "* structured? a graph like structure that enables gaussians to share data and define normals as a neigberhood. \n",
    "    * each gaussian is a vertex? \n",
    "    * an interpulated color as in plenoxels? \n",
    "    * a new gaussian that represents the interpulation and normal? could be 2D\n",
    "    * will help transperacy? \n",
    "\n",
    "\n",
    "* sparse view\n",
    "    * use rendered view for reconstruction\n",
    "    * use 3d/2d/3d with normal to render images\n",
    "    * use the new images to get a more detailed depth map and regularize the normals\n",
    "    * find \"good\" renders in image. doesnt have to be the whole image, can be patch - mesure quality\n",
    "    * we can understand what points are ocluded by Z buffer - maybe add there more points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Gaussian splatting\n",
    "## Elliptical Gaussian (from EWA article) - Math explained\n",
    "\n",
    "\n",
    "we define an elliptical gaussian: \n",
    "\n",
    "Equation 1: $G_\\mathbf{V}(\\mathbf{x} - \\mathbf{p}) = \\frac{1}{2 \\pi |\\mathbf{V}|} \\frac{1}{2} e^{-\\frac{1}{2} (\\mathbf{x} - \\mathbf{p})^T \\mathbf{V}^{-1} (\\mathbf{x} - \\mathbf{p})}.$\n",
    "\n",
    "* $V$ is the covariance matrix\n",
    "* $\\mathbf{x}$ - a 3d coordinate (in gaussian axes)\n",
    "* $\\mathbf{p}$ - the mean of the gaussian \n",
    "### Affine projection of the gauusian\n",
    "We define an arbitrary affine mapping from object space $ \\mathbf{u} = \\Phi(\\mathbf{x}) $, where $ \\Phi(\\mathbf{x}) = \\mathbf{Mx + c} $, \n",
    "\n",
    "and we substitute $ \\mathbf{x} = \\Phi^{-1}(\\mathbf{u}) $. Since $ \\Phi^{-1}(\\mathbf{u}) = \\mathbf{M}^{-1}(\\mathbf{u} - \\mathbf{c}) $, we get:\n",
    "\n",
    "$$\n",
    "G_\\mathbf{V}(\\Phi^{-1}(\\mathbf{u}) - \\mathbf{p}) = \n",
    "\\frac{1}{|M^{-1}|} \\cdot \\frac{1}{(2 \\pi)^{3/2} |\\mathbf{V}|^{1/2}} e^{-\\frac{1}{2} (\\mathbf{M}^{-1}(\\mathbf{u} - \\mathbf{c}) - \\mathbf{p})^T \\mathbf{V}^{-1} (\\mathbf{M}^{-1}(\\mathbf{u} - \\mathbf{c}) - \\mathbf{p})} = \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{|M^{-1}|} \\cdot \\frac{1}{(2 \\pi)^{3/2} |\\mathbf{V}|^{1/2}} e^{-\\frac{1}{2} (\\mathbf{u} - \\mathbf{c} - \\mathbf{Mp})^T \\mathbf{M}^{-1} \\mathbf{V}^{-1} (\\mathbf{M}^{-1})^T (\\mathbf{u} - \\mathbf{c} - \\mathbf{Mp})} = \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{|M^{-1}|} \\cdot \\frac{1}{(2 \\pi)^{3/2} |\\mathbf{V}|^{1/2}} e^{-\\frac{1}{2} (\\mathbf{u} - \\Phi(\\mathbf{p}))^T (\\mathbf{MVM}^T)^{-1} (\\mathbf{u} - \\Phi(\\mathbf{p}))}.\n",
    "$$\n",
    "\n",
    "from this, to ease the writing we define equation 2: \n",
    "\n",
    "Equation 2: $G \\mathbf{V}(\\Phi^{-1}(u) - \\mathbf{p}) = \\frac{1}{|\\mathbf{M}^{-1}|} G_{ \\mathbf{M} \\mathbf{V} \\mathbf{M}^T} (u - \\Phi(\\mathbf{p})).$\n",
    "\n",
    "#### The covariance matrix\n",
    "$V$, the covariance matrix is a symetrix matrix that can be ragarded as a transformation. it scales and shears the data (thus defining the orientation and ratio between the gaussian principal axes). \n",
    "As mentioned above, the gaussian is transformed using an affine mapping $\\Phi(X)$. Usually we use this kind of mapping to transform a vector or a 3D point to a new FoR (change basis). Because the covariance matrix is a transformation, in order to change its FoR (basis), we will perform $\\mathbf{V_{1}} = MVM^{T}$. We first multiply $VM^{T}$  - transform (whatever xyz we multiply) from camera to body ($M^{T}$) and then multiply by $V$ which is in object coordinates - now we have coordinates in object basis after shear and scale. we then multiply by $M$ again to transform back to camera coordinates. \n",
    "\n",
    "\n",
    "### Projection\n",
    "\n",
    "\n",
    "![alt text](ray_ewa.png)\n",
    "#### Ray space\n",
    "\n",
    "We converted to camera space, now we want to convert to ray space. The coordinates in ray space are defined such that $x_0, x_1$ are the coordinates on the image plane and the devision by $u_2$ adds prespective. $u_3$ is then defined as the euclidean distance from the point to the origin (camera). \n",
    "\n",
    "\n",
    "$$ \\text{ray space to camera: }\n",
    "\\begin{pmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\n",
    "\\end{pmatrix} = m(u) = \\begin{pmatrix}\n",
    "u_0 / u_2 \\\\\n",
    "u_1/u_2 \\\\\n",
    "||(u_0, u_1, u_2)||^{T} \\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\text{,camera to ray space: }\n",
    "\\begin{pmatrix}\n",
    "u_0 \\\\\n",
    "u_1 \\\\\n",
    "u_2 \\\\\n",
    "\n",
    "\\end{pmatrix} = m^{-1}(x) = \\begin{pmatrix}\n",
    "x_0/||(x_0,x_1,1)^T|| \\cdot x_2 \\\\\n",
    "x_1/||(x_0,x_1,1)^T|| \\cdot x_2 \\\\\n",
    "1/||(x_0,x_1,1)^T|| \\cdot x_2 \\\\\n",
    "\n",
    "\\end{pmatrix}\n",
    "\n",
    "\n",
    "$$\n",
    "  \n",
    "Ray space is defined with 3 coordinates $(x_0,x_1,x_2)$ where $x_0$ and $x_1$ represents the location of the pixel and $x_3$ is the euclidean distance from the camera origin to $(x_1,x_2). This transformation is not affine due to the devision by $u_2$, parallel lines are not preserved and the shape of the gaussian will be distorted. (in addition, the dimention is problematic).\n",
    "In order to solve this a local affine approximation $\\mathbf{m_{u_k}}$ of the projection transformation is defined. The approximation is the first to terms of the Tayor expansion of $\\mathbf{m}$ at point $\\mathbf{u}_{k}$:\n",
    "$$\\mathbf{m_{u_k}\\mathbf(u)} = \\mathbf x_k+\\mathbf J_{\\mathbf{u}_k}\\cdot ( \\mathbf u-\\mathbf u_k )$$\n",
    "where $\\mathbf x_k=\\mathbf m(\\mathbf u_k)$ is the mean of the gaussian in ray space (can be transformed using the mapping m, the projection matrix) and the Jacobian $ J_{u_k} = \\frac{\\partial m}{\\partial u}(u_k) $\n",
    "\n",
    "### The gaussian mapping\n",
    "#### Object to camera coordinates (as detailed in Affine projection of the gauusian)\n",
    "\n",
    "The reconstruction kernels are initially given in object space, which has coordinates $t =(t_0,t_1,t_2)^T$ . \n",
    "* Gaussian reconstruction kernels in object space: $r\"_k (t)=G_{V\"_k} (t − t_k)$, where $t_k$ are the voxel positions in object space\n",
    "* Object coordinates are transformed to camera coordinates using an affine mapping $\\mathbf u = \\phi(t)$, called viewing transformation. It is defined by a matrix $\\mathbf W$ and a translation vector $\\mathbf d$ as $\\phi(t) = Wt + d$. We transform the reconstruction kernels $G_{V\"k} (t − t_k)$ to camera space by substituting $t = \\phi ^{−1}(u)$ and using Equation 2 : \n",
    "\n",
    "$G \\mathbf{V\"_{k}}(\\Phi^{-1}(u) - t_k) = \\frac{1}{|\\mathbf{W}^{-1}|} G_{V'_k } (\\mathbf u - \\mathbf u_k) = r'_k(u\\mathbf).$\n",
    "\n",
    "where $u_k = \\phi(\\mathbf t_k)$ is the center of the Gaussian in camera coordinates and $r′_k(\\mathbf u)$ denotes the reconstruction kernel in camera space. According to equation 2, the covariance matrix in camera coordinates $V′_k$ is given by $V′_k = WV\"_kW^T$ (exactly as detailed in Affine projection of the gauusian).\n",
    "\n",
    "#### Camera to ray space\n",
    "Reminder: the local affine transformation - $$\\mathbf{m_{u_k}\\mathbf(u)} = \\mathbf x_k+\\mathbf J_{\\mathbf{u}_k}\\cdot ( \\mathbf u-\\mathbf u_k )$$\n",
    "$\\mathbf{m_{u_k}\\mathbf(u)}$ transforms from camera to ray, hence $x_k=m(u_k)$ where $x_k$ is the center of the gaussian in ray space. \n",
    "since $\\mathbf x = m(\\mathbf u)$ we get that $ u = \\mathbf m^{-1}(\\mathbf(x))$ hence: \n",
    "$$ u = \\mathbf m^{-1}(x) = \\mathbf(x - x_k)\\cdot \\mathbf J^{-1}_{\\mathbf u_k} + \\mathbf u_k $$\n",
    "$$ r_k(\\mathbf x) = \\frac{1}{|\\mathbf W^{-1}|}\\cdot G_{\\mathbf V'_k}(\\mathbf{(x - x_k)\\cdot J^{-1}_{\\mathbf u_k} + \\mathbf u_k - \\mathbf u_k}) = \\\\\n",
    "\\frac{1}{|\\mathbf W^{-1}||\\mathbf J^{-1}|}G_{\\mathbf V'_k}(J^{-1}_{\\mathbf u_k}\\mathbf{(x - x_k)})\\\\$$\n",
    "\n",
    "and the new covariance matrix is defined:  $Vk = JV'_{k}$ $J^T = JWV\"_kW^TJ^T$.\n",
    "where V\"_k is the original covariance matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Perspective Projection and Jacobian Derivation\n",
    "\n",
    "In a pinhole camera model, the perspective projection of a 3D point \\((X, Y, Z)\\) into 2D image coordinates \\((x, y)\\) can be expressed as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "1\n",
    "\\end{bmatrix} = \\frac{1}{Z} \\cdot\n",
    "\\begin{bmatrix}\n",
    "f_x \\cdot X + c_x \\cdot Z \\\\\n",
    "f_y \\cdot Y + c_y \\cdot Z \\\\\n",
    "Z\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $(x, y)$  : are the projected 2D coordinates on the image plane.\n",
    "* $(X, Y, Z)$  : are the coordinates in the 3D space. \n",
    "* $f_x , f_y$  : are the focal lengths in the x and y directions, respectively. \n",
    "* $c_x , c_y$  : are the coordinates of the principal point (optical center) in the image. \n",
    "\n",
    "### Derivation of the Jacobian\n",
    "\n",
    "To derive the Jacobian, we compute the partial derivatives of the projected 2D coordinates \\((x, y)\\) with respect to the 3D coordinates \\((X, Y, Z)\\).\n",
    "\n",
    "1. **From the perspective projection equations**:\n",
    "   - For \\(x\\):\n",
    "   $$\n",
    "   x = \\frac{f_x \\cdot X + c_x \\cdot Z}{Z}\n",
    "   $$\n",
    "   - For \\(y\\):\n",
    "   $$\n",
    "   y = \\frac{f_y \\cdot Y + c_y \\cdot Z}{Z}\n",
    "   $$\n",
    "\n",
    "2. **Calculating the derivatives**:\n",
    "   - The derivatives with respect to \\(Z\\) yield:\n",
    "     - For \\(x\\):\n",
    "     $$\n",
    "     \\frac{\\partial x}{\\partial X} = \\frac{f_x}{Z}, \\quad \\frac{\\partial x}{\\partial Y} = 0, \\quad \\frac{\\partial x}{\\partial Z} = -\\frac{f_x \\cdot X}{Z^2}\n",
    "     $$\n",
    "     - For \\(y\\):\n",
    "     $$\n",
    "     \\frac{\\partial y}{\\partial X} = 0, \\quad \\frac{\\partial y}{\\partial Y} = \\frac{f_y}{Z}, \\quad \\frac{\\partial y}{\\partial Z} = -\\frac{f_y \\cdot Y}{Z^2}\n",
    "     $$\n",
    "\n",
    "3. **Forming the Jacobian**:\n",
    "   Using the derivatives calculated above, the Jacobian matrix \\(J\\) becomes:\n",
    "   $$\n",
    "   J = \\begin{bmatrix}\n",
    "   \\frac{f_x}{Z} & 0 & -\\frac{f_x \\cdot X}{Z^2} \\\\\n",
    "   0 & \\frac{f_y}{Z} & -\\frac{f_y \\cdot Y}{Z^2}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "\n",
    "### 2D Projection \n",
    "We calculate the conics coeffitient for each gaussian : $$Ax^2+Bxy+Cy^2 = 0$$, where $A = \\frac{\\sigma_y^2}{|V|}$, $B = \\frac{\\sigma_xy}{|V|}$ and  $C = \\frac{\\sigma_x^2}{|V|}$\n",
    "we then multiply by the distace between the gaussiam and the pixel. this is the same as: \n",
    "$-\\frac{1}{2} (\\mathbf{x} - \\mathbf{p})^T \\mathbf{V}^{-1} (\\mathbf{x} - \\mathbf{p})$ - the exponent of the gaussian\n",
    "\n",
    "wIncorporating the scaled conic, which represents the 2D Gaussian as an ellipse in the Gaussian equation, we can express the effect of every Gaussian on the pixel's color as follows:\n",
    "\n",
    "$$\n",
    "G_{\\mathbf{V}}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{3/2} |\\mathbf{V}|^{1/2}} e^{-\\frac{1}{2} ( Ax^2+Bxy+Cy^2)}\n",
    "$$\n",
    "\n",
    "This equation captures how each Gaussian contributes to the pixel's color based on its distance from the pixel and the shape defined by the covariance matrix.\n",
    "\n",
    "## tiles\n",
    "To render the image we devide it to tiles of $ 16 X 16 $, for each tile we sort the gaussians according to the distance from the camera. we then sample all gaussians effect per pixel in the tile (using the power equation to calculate the opacity). We sum the gaussians color values and multiply by the opacity (doing alpha blending, each time we update the opacity according to the already calculated gaussians $color = RGB * \\alpha *\\alpha_{blend})$ were for eacg gaussian we add we update $\\alpha_{blend} =  \\alpha_{blend}*(1-\\alpha) $ (when we reach 0, its opaque). to later do the alpha blend we pick for each tile only the gaussians that intersect with the tile. since gaussians are not bounded, we bound them with 3 $\\sigma_x$ and 3 $\\sigma_y$ bounding box in each axis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Gaussian splatting\n",
    "\n",
    "## 2DGS - Math explained\n",
    "### Modeling: \n",
    "The 2D Gaussian is defined in terms of its local coordinate system, where the $ X$ and $ Y$ axes are scaled according to the Gaussian's shape. The Gaussian is represented in the **world coordinates**, with the **axes** of the Gaussian in world space defined by the tangential vectors $ \\mathbf{t_u} $ and $ \\mathbf{t_v} $, and its **scaling factors** by $ s_u $ and $ s_v $. These tangential vectors define the directions of the local coordinate axes in the tangent plane (the object FoR).\n",
    "\n",
    "The **object plane**, which is the tangent plane to the Gaussian in world space, can be described using the plane equation:\n",
    "$$\n",
    "P(u, v) = p_k + s_u \\mathbf{t_u} u + s_v \\mathbf{t_v} v\n",
    "$$\n",
    "where $ p_k $ is the center of the Gaussian in world coordinates, and $ u $ and $ v $ represent local coordinates on the tangent plane. This equation expresses the position of any point in the tangent plane in terms of the world coordinates, as modified by the scaling along the tangential axes $ \\mathbf{t_u} $ and $ \\mathbf{t_v} $.\n",
    "\n",
    "To transform the local coordinates in the tangent plane (object frame of reference) to world coordinates, we define the following transformation matrix $ H $, which encodes the **scaling**, **rotation**, and **translation** from the object space to the world space:\n",
    "$$\n",
    "H = \\begin{pmatrix}\n",
    "s_u \\mathbf{t_u} & s_v \\mathbf{t_v} & 0 & p_k \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "R & p_k \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where:\n",
    "- $ \\mathbf{t_u} = \\begin{pmatrix} t_{u_x} \\\\ t_{u_y} \\\\ t_{u_z} \\end{pmatrix} $ and $ \\mathbf{t_v} = \\begin{pmatrix} t_{v_x} \\\\ t_{v_y} \\\\ t_{v_z} \\end{pmatrix} $ are the 3D tangential vectors (defining the axes of the tangent plane in world coordinates),\n",
    "- $ p_k = \\begin{pmatrix} p_{k_x} \\\\ p_{k_y} \\\\ p_{k_z} \\end{pmatrix} $ is the 3D position of the Gaussian center in world coordinates, and\n",
    "- $ R $ is the 3x3 rotation matrix that describes the orientation of the tangent plane in world space.\n",
    "\n",
    "The matrix $ H $ can be interpreted in two parts:\n",
    "- The first part, $ \\begin{pmatrix} s_u \\mathbf{t_u} & s_v \\mathbf{t_v} & 0 \\end{pmatrix} $, represents the scaling and rotation of the axes in the world coordinates. It scales the tangential vectors $ \\mathbf{t_u} $ and $ \\mathbf{t_v} $ by $ s_u $ and $ s_v $, respectively, and applies any rotational transformation.\n",
    "- The second part, $ p_k $, represents the **translation** of the Gaussian's center in world coordinates, which shifts the origin of the local tangent plane to the desired world position.\n",
    "\n",
    "The matrix can also be interpreted in terms of the rotation matrix $ R $ (which describes the orientation of the tangent plane in world space) and the translation vector $ p_k $. The transformation from local coordinates $ (u, v) $ to world coordinates $ (x, y, z) $ is thus achieved through the multiplication of $ H $ by the local coordinate vector:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "= H \\begin{pmatrix}\n",
    "u \\\\\n",
    "v \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This transformation allows us to map a point in the tangent plane (object space) to the corresponding point in the world space, accounting for both the Gaussian's shape (through the scaling $ s_u, s_v $) and its orientation (through the rotation $ R $).\n",
    "\n",
    "In summary, the matrix $ H $ serves as a **homogeneous transformation** that encapsulates both the geometric properties (scaling and rotation) and the positioning (translation) of the 2D Gaussian in world space.\n",
    "\n",
    "for every u,v on the tangent plane we can calculate the gaussian power: \n",
    "$$ G(u) = \\exp\\left( -\\frac{u^2 + v^2}{2} \\right) \\tag{6} $$\n",
    "\n",
    "* $ \\mathbf{t_u} $ , $ \\mathbf{t_v} , $ $\\mathbf{s_u} $ and $\\mathbf{s_v}$ are learnable parameters.\n",
    "* each gaussian is defined by the opacity $\\alpha$ and a view dependent color.\n",
    "\n",
    "### Splatting\n",
    "\n",
    "\n",
    "The goal of this process is to find the intersection between a ray, originating from a pixel in the image, and the tangent plane of a 2D Gaussian, and then evaluate the power of the Gaussian at the intersection point. Below are the detailed steps.\n",
    "\n",
    "Step 1: Defining the Image Ray \n",
    "\n",
    "We begin by defining the ray from a pixel in the image using two orthogonal planes. The pixel location $(x, y) $ in the screen space can be used to define two planes:\n",
    "\n",
    "\n",
    "* The x-plane (yz): a plane defined by a normal vector $ \\mathbf{n}_x = (-1, 0, 0) $ and an offset $ x $. The 4D homogeneous form is $ h_x = (-1, 0, 0, x) $.\n",
    "* The y-plane (xz): a plane defined by a normal vector $ \\mathbf{n}_y = (0, -1, 0) $ and an offset $ y $. The 4D homogeneous form is $ h_y = (0, -1, 0, y) $.\n",
    "\n",
    "\n",
    "These planes intersect at a ray in 3D space that is represented as the line of intersection between the two planes.\n",
    "\n",
    "Step 2: Transforming the Planes to the Tangent Plane\n",
    "\n",
    "Next, we transform the planes from the image space into the local coordinates of the 2D Gaussian primitive (the tangent plane). Using the transformation matrix $ M = (WH)^{-1} $, we apply the inverse transpose to the planes (W is the prespective projection matrix (transformation from camera to screen space), H is a transformation from tangant plane to camera space):\n",
    "\n",
    "multiplying W (camera to screen) by H (tangent plane to camera) will transform a coordinate from tangent plane to camera to screen. Transposing it will map from screen space to tangent plane \n",
    "\n",
    "$$\n",
    "h_u = (WH)^\\top h_x \\\\\n",
    "h_v = (WH)^\\top h_y\n",
    "$$\n",
    "* those are the planes rotated from image space to tangent space\n",
    "\n",
    "\n",
    "- We can imagine a ray from the camera center through a pixel, The pixel is described in screen space (here we use NDC) which means we transform it twice. \n",
    "1. from NDC to camera coordinate\n",
    "2. from camera coordinates to gaussian (objec) coordinates. \n",
    "* The ray stays the same, it is still begins in the camera center and pass through the pixel. After all transformations we have the ray as seen from the gaussian axes. \n",
    "\n",
    "Step 3: Solving for the Intersection\n",
    "\n",
    "The next step is to solve for the intersection point $(u, v) $ of the ray with the transformed tangent plane. We do this by solving the following system of equations:\n",
    "\n",
    "$$\n",
    "h_u \\cdot (u, v, 1, 1)^\\top = 0 \\\\\n",
    "h_v \\cdot (u, v, 1, 1)^\\top = 0\n",
    "$$\n",
    "\n",
    "Expanding these equations gives us:\n",
    "\n",
    "\n",
    "$$\n",
    "    h_1^u u + h_2^u v + h_3^u + h_4^u = 0 \\\\\n",
    "    h_1^v u + h_2^v v + h_3^v + h_4^v = 0\n",
    "$$\n",
    "\n",
    "\n",
    "The solution for $ u $ and $ v $ is obtained by solving this system of equations. This leads to the following expressions for the coordinates $ u(x) $ and $ v(x) $:\n",
    "\n",
    "$$\n",
    "u = \\frac{h_2^u h_4^v - h_4^u h_2^v}{h_1^u h_2^v - h_2^u h_1^v} \\\\\n",
    "v = \\frac{h_4^u h_1^v - h_1^u h_4^v}{h_1^u h_2^v - h_2^u h_1^v}\n",
    "$$\n",
    "\n",
    "Where $ h_i^u $ and $ h_i^v $ are the homogeneous parameters of the transformed planes.\n",
    "\n",
    "Step 4: Evaluating the Gaussian at the Intersection\n",
    "\n",
    "Once we have the coordinates $ (u, v) $, we can evaluate the 2D Gaussian function at the intersection point. The Gaussian function is typically of the form:\n",
    "\n",
    "$$\n",
    "G(u, v) = \\exp\\left( -\\frac{u^2 + v^2}{2} \\right)\n",
    "$$\n",
    "\n",
    "This represents the Gaussian value at the point $ (u, v) $ on the tangent plane.\n",
    "\n",
    "\n",
    "Once the values of $ u(x) $ and $ v(x) $ are found, we can compute the depth of the intersection point using the following equation:\n",
    "$$\n",
    "x = (x_z, y_z, z, z)^\\top = W P(u, v) = W H (u, v, 1, 1)^\\top\n",
    "$$\n",
    "Here, $ W $ is the camera projection matrix, and $ H $ is the transformation matrix from the tangent plane to world coordinates. The last component of the resulting vector $ x = (xz, yz, z, z)^\\top $ gives the depth $ z $ of the intersection point (the third element, need to make sure). \n",
    "\n",
    "\n",
    "\n",
    "#### Screen space\n",
    "this transformation returns a 3d coordinate in screen space. \n",
    ", where rendering occures. the projection matrix transformes x,y,z to the screen space\n",
    "* x axis spans from 0 to -1 (width)\n",
    "* y axis spans from 0 to -1 (hight)\n",
    "* Z_{NDC} represents the depth (need to check that)\n",
    "from NDC we can map to pixels\n",
    "\n",
    "#### Transforming planes\n",
    "* A point in 3D homogeneous coordinates is represented as $ (x,y,z,w)\n",
    "* a plane consist of both a normal and an offset - in 3D homogeneous coordinates it is represented as $(a,b,c,d)\n",
    "where $(a,b,c) is the planes normal vector and $d$ is its offset. \n",
    "\n",
    "The plane equation : $ ax + by + cz + dw = 0 $\n",
    "which means that that dot product of the plane parameters $ (a,b,c,d)$ and a point $(x,y,z,w)$ is zero for a point lying on the plane. \n",
    "\n",
    "** Transforming Point on a Plane **\n",
    "we Define matrix $M$ - a transformation matrix. \n",
    "tansforming a point on the plain using $M$: $p'=M\\cdot p$\n",
    "after transforming it, the plane equation should still hold: $h'p' = 0$ were $h', p'$ are the transformed plane and point parameters.\n",
    "\n",
    "Because $p' = M\\cdot p$ we get that $h'(M\\cdot p) = 0 $ and since $ h' \\cdot(M\\cdotp p)=M^T \\cdot h' \\cdot p $ we can write \n",
    "$$\n",
    "M^T \\cdot h' \\cdot p = h\\cdot p \\\\\n",
    "M^T \\cdot h' = h\n",
    "$$\n",
    "\n",
    "and from that we show that the plane $h'$ remains valid after transformation. \n",
    "\n",
    "\n",
    "#### Regularization terms\n",
    "\n",
    "There are two regularization terms, the first Depth distortion and the second - Normal consistency.\n",
    "\n",
    "\n",
    "### Normal consistency: \n",
    "\n",
    "$ L_n = \\sum_i \\omega_i (1 - \\mathbf{n}_i^\\top \\mathbf{N})$\n",
    "\n",
    "* n - the normal of the 2d gaussian, we calculate it by rotating the Z axis from world axes to camera axes. \n",
    "\n",
    "* N - the normal to the surface. we consider the actual surface at the median point of intersection, where the accumulated opacity reaches 0.5. \n",
    "The sampled point generate a 2d depth map (prespective projection). We then transform it to 3d surface in world coordinates. \n",
    "To do so we calculate a ray from the camera center to through the pixel. We transform it from the 2d image space to the camera space (3d) to the world space (3D). We then multiply each ray direction by the depth value at the corresponding pixel to \"scale\" the ray to the correct length, reaching the surface point. \n",
    "After calculating the surface we can calculate the gradients in X and Y direction, thus getting the normal to the tangent plane for every voxel of the surface. (finaly we normalize by this normal).\n",
    "* $\\omega_i$ - the blending weight of the i-th intersection\n",
    "\n",
    "This term will try to take the dot product of both normals to 1, thus agjusting the splats to locally align with the surface\n",
    "* the input depth map here is generated from the coordinates of the mean of the gaussians. Hence, if the gaussians are on the surface (achieved by the othe regularization term) then we will have a general estimation of the normals. \n",
    "\n",
    "### Depth distortion: \n",
    "\n",
    "$ L_d = \\sum_{i,j} \\omega_i \\omega_j |z_i - z_j| $\n",
    "\n",
    "* $\\omega_{i/j}$ - the blending weight of the i-th intersection - close to one is opaque, close to zero is opaque, close to one is transperant. \n",
    "* $ z_i  $ - the intersection between a ray from the camera origin through the pixel to the splat. $ |z_i - z_j| $ is the distance between the intersection points. \n",
    "\n",
    "\n",
    "The term will make the splats close to each other and opaque - hence optimizing the surface of the object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSDF (used for mesh reconstruction)\n",
    "* define a voxel grid, each voxel will contain TSDF value and weight\n",
    "* for each view identify the voxels that are inside the frustum\n",
    "* foar each of the voxels project to the depth map and sample the depth\n",
    "* calculate the SDF between the voxel's depth and the measured depth from the image. \n",
    "* Retrieve the current TSDF value (TSDF_old) and weight (weight_old) stored in the voxel.\n",
    "* calculate the new TSDF and Weight and update for the voxel:\n",
    "    * $TSDF_{new weighted} = (TSDF_{old} * weight_{old} + TSDF_{new} * 1) / (weight_{old} + 1)$\n",
    "    * $weight_{new total} = weight_{old} + 1$\n",
    "* Store the TSDF_{new_weighted} and weight_{new_total} back into the voxel\n",
    "\n",
    "### Reconstruct the surface\n",
    "* Identify voxels where the absolute value of the TSDF is below a small threshold (e.g., 0.1). These voxels are considered to be part of the surface.\n",
    "* Optionally, use algorithms like Marching Cubes to generate a continuous mesh representation of the surface from the TSDF volume."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
